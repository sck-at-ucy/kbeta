[![CIÂ (macOSÂ arm64)](https://github.com/sck-at-ucy/kbeta/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/sck-at-ucy/kbeta/actions/workflows/ci.yml)

<p align="center">
  <img src="assets/MLX_Kourkoutas.png" width="600"/>
</p>

# kbetaÂ â€“Â *Kourkoutasâ€‘Î² Optimiser* Â Â ğŸŒğŸ¦ğŸš€ğŸ“ˆ

> Reference implementation of **Kourkoutasâ€‘Î²: A Sunspikeâ€‘Driven Adam Optimizer with Desert Flair**
> Published as [arXiv:2508.12996](http://arxiv.org/abs/2508.12996).

This repository provides the optimiser implementation together with example workloads for reproducibility.

---

## Tableâ€¯ofâ€¯Contents
1. [Key ideas](#key-ideas)
2. [Project layout](#project-layout)
3. [Quick start](#quick-start)
4. [Installation](#installation)
5. [Using Kourkoutasâ€‘Î² in your own model](#minimal-example)
6. [Example workloads](#example-workloads)
7. [Dataset and creation (verifiable)](#dataset-and-creation-verifiable)
8. [Model and training protocol](#model-and-training-protocol)
9. [Optimizers and settings](#optimizers-and-settings)
10. [Companion repositories](#companion-repositories)
11. [Tests & linting](#tests--linting)
12. [Citation](#citation)
13. [License](#license)
14. [Contributing & roadmap](#contributing--roadmap)

---

## Key ideas

* **Layerâ€‘wise dynamic Î²â‚‚** driven by a bounded *sunâ€‘spike* signal (gradient norm vs. EMA).
* **Two Î²â‚‚ parameters**: *Î²â‚‚_min* for agility under spikes, *Î²â‚‚_max* for stability when calm.
* **Optional features**: softâ€‘max AMSGrad, trustâ€‘region clipping, adaptive tiny term.
* **Dropâ€‘in compatibility**: recovers **exact Adam** when dynamic Î²â‚‚ and extras are disabled.
* 100â€¯% **Apple MLX** compatible â€“ no PyTorch required.

See the paper for derivations, experiments, and theoretical analysis.

---

## Conceptual overview

### Highâ€‘level intuition â€“ the â€œdesert lizardâ€ view
*Kourkoutasâ€‘Î²* is an Adamâ€‘style optimiser whose secondâ€‘moment decay **Î²â‚‚** is no longer a hardâ€‘wired constant.
Instead, every update computes a **sunâ€‘spike score**â€”a single, cheap scalar that compares the current gradient magnitude to its exponentiallyâ€‘weighted history.  We then **map that score to Î²â‚‚ on the fly**:

| Sunâ€‘spike | Lizard metaphor | Adaptive behaviour |
|-----------|-----------------|--------------------|
| **High**  | The desert sun is scorching â€” the lizard is â€œfully warmed upâ€ and sprints. | **Lower Î²â‚‚ toward Î²â‚‚,min** â†’ secondâ€‘moment memory shortens, allowing rapid, large parameter moves. |
| **Low**   | Itâ€™s cool; the lizard feels sluggish and takes cautious steps. | **Raise Î²â‚‚ toward Î²â‚‚,max** â†’ longer memory, filtering noise and producing steadier updates. |

Because the sunâ€‘spike diagnostic **exists only in Kourkoutasâ€‘Î²**, the method can be viewed as *Adam with a temperatureâ€‘controlled Î²â‚‚ schedule*: warm gradients trigger exploration; cooler gradients favour exploitation and stability.

---

## Project layout

```
kbeta
â”œâ”€â”€ src/kbeta/                   # pip package
â”‚Â Â  â”œâ”€â”€ __init__.py              # exports KourkoutasBeta / KourkoutasSoftmaxFlex
â”‚Â Â  â””â”€â”€ optim/
â”‚Â Â      â””â”€â”€ kbeta_softmax.py     # implementation
â”‚
â”œâ”€â”€ examples/
â”‚Â Â  â””â”€â”€ transformer_char_lm/     # TestbedÂ D: characterâ€‘level LM on smallâ€‘enwik8
â”‚
â”œâ”€â”€ tests/                       # pytest suite (smoke + ablation tests)
â”œâ”€â”€ assets/                      # logo and figure
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ MANIFEST.in
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## Quick start

```bash
# 1. clone your fork
git clone git@github.com:<YOUR-USERNAME>/kbeta.git
cd kbeta

# 2. create a fresh virtualenv
python -m venv .venv && source .venv/bin/activate

# 3. editable install + dev extras
pip install -e ".[dev]"

# 4. run the smoke + ablation tests
pytest -q
```
---

## Installation

### Option 1: PyPI wheels (end-users)

If you only want the optimiser in your own MLX projects, install from PyPI:

```bash
pip install kbeta
```

This gives you just the `kbeta` package with the latest MLX.

For development tools and examples:

```bash
pip install "kbeta[dev]"
```

For exact reproducibility of the paper results (MLX 0.26.3, Adam-95/999 baselines):

```bash
pip install "kbeta[repro]"
```

---

### Option 2: Cloning the repo (researchers / contributors)

If you want to run the example workloads or contribute to development, clone the repo:

```bash
git clone https://github.com/sck-at-ucy/kbeta.git
cd kbeta
python -m venv .venv && source .venv/bin/activate
pip install -e ".[dev]"
```

This installs the package in editable mode and makes all example scripts available.

---

## Minimal example

```python
import time
import mlx.core as mx
import mlx.nn as nn
from kbeta import KourkoutasBeta

num_features, num_examples, num_iters, lr = 100, 1000, 1000, 0.01

# True parameters and data
w_star = mx.random.normal((num_features,))
X = mx.random.normal((num_examples, num_features))
y = X @ w_star + 1e-2 * mx.random.normal((num_examples,))

# Simple model with one parameter
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.w = mx.zeros((num_features,))

    def __call__(self, x):
        return x @ self.w

model = Model()

def loss_fn(m):
    return 0.5 * mx.mean(mx.square(m(X) - y))

opt = KourkoutasBeta(learning_rate=lr)
opt.init(model.parameters())

grad_fn = nn.value_and_grad(model,loss_fn)

tic = time.time()
for _ in range(num_iters):
    loss, grads = grad_fn(model)
    opt.update(model, grads)
    mx.eval(model.parameters())
toc = time.time()

error_norm = float(mx.linalg.norm(model.w - w_star))
print(f"Loss={loss.item():.5f}, L2|w-w*|={error_norm:.5f} "
      f"Throughput={num_iters/(toc-tic):.1f} it/s")
```

---

## Example workloads

**Important:** ğŸ‘‰ ğŸ‘‰ The 2â€‘D Transformer (Heat2D, TestbedÂ A) and 3â€‘D PINN (Heat3D, TestbedÂ B) of the paper are released as separate repositories:
- [kbeta-transformer2d](https://github.com/sck-at-ucy/kbeta-transformer2d)
- [kbeta-pinn3d](https://github.com/sck-at-ucy/kbeta-pinn3d)

This repo includes the Transformer â€“ Testbed D (Char-level LM on small-enwik8)

| Folder | Paper section | What it shows | How to run |
|--------|---------------|---------------|------------|
| `examples/transformer_char_lm` | Â§â€¯6.4 (TestbedÂ D) | Characterâ€‘level LM on *smallâ€‘enwik8* | `python examples/transformer_char_lm/testbed_d.py --text ./data/small_enwik8.txt --opt kbeta` |


### Running Transformer â€“ Testbed D (Char-level LM on small-enwik8)

All commands assume running from the **repo root** (adjust accordingly)
ğŸ‘‰ Make sure you have generated `./data/small-enwik8.txt` and the `./logs_enwi` directory as described below.

Run the Transformer training with the same options used in the paper (adapted to the repo paths):

```bash
  python -u examples/transformer_char_lm/testbed_d.py --text ./data/small-enwik8.txt     --steps 50001 --batch 4 --d_model 512 --n_layer 6 --n_head 8     --ctx 512 --lmin 16 --lmax 512 --warmup 250 --opt kbeta --adam_beta2 0.95     --layer_bucket per-array --barrier_every 100 --eval_every 500     --lr 1e-3     --seed 0 --fixed_eval_seed 1234 --deterministic --compile     --wd 0.0 --lr_schedule "1:1e-3,30000:5e-4,40000:1e-4,60000:1e-5"     2>&1 | tee "logs_enwik/kbeta_seed0.log"
```

This reproduces a run that mirros the testbed reported in the paper with full logging under `logs_enwik/`.

---

### Dataset and creation (verifiable)

We use the **first 30 MB of enwik8** (the classic Hutter Prize corpus).
The slice is created deterministically:

```bash
curl -L -o enwik8.zip https://data.deepai.org/enwik8.zip
unzip enwik8.zip
head -c 30000000 enwik8 > small-enwik8.txt
mkdir -p data && mv small-enwik8.txt data/
mkdir ./logs_enwik
```

Checksums on our machine:

```bash
sha256sum enwik8
# 2b49720e...c024a8

sha256sum data/small-enwik8.txt
# e0152eee...298b7
```

Re-creating `small-enwik8.txt` reproduced the same SHAâ€‘256 (bitâ€‘forâ€‘bit identity).

---

### Model and training protocol

As in the provided script, we train:

* **Architecture**: 6â€‘block Transformer (`d_model=512`, `n_head=8`, FFN width = 4d)
  GELU, LayerNorm, causal selfâ€‘attention; no dropout or weight decay.
* **Data schedule**: variable sequence length with deterministic bucketing
  \(L \in [16,512]\), rounded to multiples of 32; batch = 4; context window = 512.
* **Steps**: 50,001
* **Learning rate schedule**:
  - 1eâ€‘3 for steps 1 â‰¤ s < 30k
  - 5eâ€‘4 for 30k â‰¤ s < 40k
  - 1eâ€‘4 for 40k â‰¤ s â‰¤ 50k
* **Evaluation**: fixed heldâ€‘out batch (length = 256, B = 128) reporting crossâ€‘entropy and BPC.
* **Runs**: 10 matched seeds (0â€“9).

---

### Optimizers and settings

- **Kourkoutasâ€‘Î² (ours)**:
  Î²â‚=0.9; dynamic Î²â‚‚âˆˆ[0.88,0.999]; Î±=0.93 (EMA for sunspike); Îµ=1eâ€‘8;
  warmâ€‘up=250 steps; `bias_correction="beta2max"`; perâ€‘array stable buckets;
  no AMSGrad/clip/adaptiveâ€‘tiny; diagnostics off.

- **Adamâ€‘95**:
  MLX Adam (Î²â‚=0.9, Î²â‚‚=0.95, Îµ=1eâ€‘8), bias correction on.

- **Adamâ€‘999**:
  MLX Adam (Î²â‚=0.9, Î²â‚‚=0.999, Îµ=1eâ€‘8), bias correction on.

---
## Companion repositories

This repository hosts the **core optimizer implementation** and the **char-level Transformer example** (Testbed D).

Other workloads from the paper are available in dedicated repositories:

- [**kbeta-transformer2d**](https://github.com/sck-at-ucy/kbeta-transformer2d) â€“ 2-D Transformer surrogate for Heat2D (Testbed A).
- [**kbeta-pinn3d**](https://github.com/sck-at-ucy/kbeta-pinn3d) â€“ 3-D Physics-Informed Neural Network for Heat3D (Testbed B).

These companion repos share the same optimizer API and training protocol, so you can directly apply `KourkoutasBeta` with no code changes.

---
## Tests & linting

```bash
pytest                 # unit & ablation tests
ruff check .           # style / imports / naming
pre-commit run --all   # run all hooks (if installed)
```

Continuous Integration (CI) runs these checks automatically.

---

## Citation

If you use this code or method in your research, please cite:

```bibtex
@article{Kassinos2025Kourkoutas,
  title   = {Kourkoutas-Î²: A Sunspike-Driven Adam Optimizer with Desert Flair},
  author  = {Stavros Kassinos},
  journal = {arXiv preprint arXiv:2508.12996},
  year    = {2025},
  url     = {http://arxiv.org/abs/2508.12996}
}
```

---

## License

This work is distributed under the **MIT License**â€”see [`LICENSE`](LICENSE) for details.

---

## Contributing & roadmap

We welcome issues & PRs!

Planned milestones:

1. **v0.1.0** â€“ optimiser + charâ€‘LM demo (public).
2. **v0.2.0** â€“ PDE workloads migrated to their own repos.
3. **v1.0.0** â€“ journal publication, pip wheels for macOS/Apple Silicon & Linux.

Happy sprinting in the (numerical) desert ğŸŒğŸ¦ğŸš€ğŸ“ˆ
