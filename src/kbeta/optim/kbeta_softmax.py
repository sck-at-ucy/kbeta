"""
Kourkoutasâ€‘Î²Â â€”Â An Adamâ€‘style optimiser with dynamicÂ Î²â‚‚ (â€œsunâ€‘spikeâ€) logic
=========================================================================

Author   :Â Stavrosâ€¯Kassinos
Firstâ€¯rev:Â Aprâ€¯2025Â Â â€“Â Â Thisâ€¯rev:Â Augâ€¯2025 (â€œsoftmaxâ€‘flexâ€ release)

-------------------------------------------------------------------------------
Desertâ€‘lizard intuitionÂ â˜€ï¸ğŸ¦
-------------------------------------------------------------------------------
Picture a **Kourkoutas**, the quick silver-gold lizard endemic to Cyprus.

* **Blazing noon â€“ Sun spikes**
  The ground is scorching, the lizard darts erratically to keep its feet cool.
  â‡’â€¯Gradient variance is **high** â†’ Î²â‚‚ is *lowered* so the optimiser reacts
  faster (less momentum smoothing, more exploration).

* **Mild morning / dusk**
  The sand is cool, the lizard moves in long, measured strides.
  â‡’â€¯Gradient variance is **low** â†’ Î²â‚‚ gravitates toward **Î²â‚‚â‚maxâ‚**
  (behaviour converges to vanilla Adam for steady refinement).

Implementation
~~~~~~~~~~~~~~
For each layer we keep an EWMA of the gradient norm, `grad_ema`.

A â€œsunâ€‘spikeâ€ scalar, using a bounded squash:

    raw = â€–gâ€– / (grad_ema + tiny_spike)      âˆˆ (0, âˆ)
    sun = raw / (1 + raw)                    âˆˆ [0, 1)

During warmâ€‘up (step â‰¤ warmup_steps), we hold sun = 0 and set
Î²â‚‚ = Â½(Î²â‚‚_min + Î²â‚‚_max). After warmâ€‘up:

    Î²â‚‚ = Î²â‚‚_max âˆ’ (Î²â‚‚_max âˆ’ Î²â‚‚_min) Â· sun

Low *sun* â‡’ Î²â‚‚Â â‰ˆÂ Î²â‚‚_max (conservative)
High *sun* â‡’ Î²â‚‚Â â‰ˆÂ Î²â‚‚_min (agile)

-------------------------------------------------------------------------------
Key additions over AdamÂ /Â AMSGrad
-------------------------------------------------------------------------------
â€¢ Layerâ€‘wise sunâ€‘spike Î²â‚‚ (see above).
â€¢ **Softâ€‘max AMSGrad** (`decayâˆˆ(0,1]`) â€“ gently leaks the running vâ€‘max buffer.
â€¢ **Trustâ€‘region clip** (`max_ratio`) â€“ caps |Î”Î¸| â‰¤ lrÂ·max_ratio.
â€¢ **Adaptive tiny term** (`adaptive_tiny`) â€“ scalesâ€¯*eps* with âŸ¨|Î¸|âŸ©.
â€¢ **Diagnostics toggle** (`diagnostics`) â€“ ultraâ€‘cheap perâ€‘epoch stats
  for plotting (sunâ€‘spike, Î²â‚‚, denomÂ min, etc.).

All toggles default to **off**, so *Kourkoutasâ€‘Î² collapses to Adam* when
`beta2_min == beta2_max == 0.999` and extras are disabled.

-------------------------------------------------------------------------------
When to try it
-------------------------------------------------------------------------------
âœ…Â PDE & physicsâ€‘informed netsâ€ƒÂ âœ…Â small / noisy dataâ€ƒÂ âœ…Â spiky gradients
âŒÂ Huge, wellâ€‘conditioned vision/LN tasks where plain Adam already excels

-------------------------------------------------------------------------------
Quickâ€‘start snippets
-------------------------------------------------------------------------------
**PINN setting**

```python
from kbeta.optim import KourkoutasSoftmaxFlex as KÎ²

opt = KÎ²(
    learning_rate = lr_schedule,
    beta1         = 0.90,
    beta2_max     = 0.999,                 # calm coasting
    beta2_min     = 0.88,                  # agile under spikes
    eps           = 1e-8,
    alpha         = 0.93,                  # EWMA for grad_ema
    tiny_spike    = 1e-9,
    tiny_denom    = 1e-8,
    decay         = 0.98,                  # softâ€‘max AMSGrad
    adaptive_tiny = True,
    max_ratio     = 3,
    bias_correction = "beta2max",
    layer_key_fn  = lambda p: p.shape,
    diagnostics   = True,                  # enables snapshot helpers
)
```

**Transformer setting**

```python
from kbeta.optim import KourkoutasSoftmaxFlex as KÎ²

opt = KÎ²(
    learning_rate = 1e-3,
    beta1         = 0.90,
    beta2_max     = 0.999,                  # calm coasting
    beta2_min     = 0.88,                   # agile under spikes
    eps           = 1e-8,
    alpha         = 0.93,
    adaptive_tiny = False,                  # often off for Transformer stacks
    layer_key_fn  = lambda p: p.shape,
    warmup_steps  = 350,
    diagnostics   = ARGS.kour_diagnostics,  # enables snapshot helpers
)
```

Inside your training loop you can call
spikes, betas = opt.snapshot_sunspike_history()
to feed violin/heatâ€‘map plots.

Happy scurrying!  â€“â€¯Stavros
"""

from collections.abc import Callable
from typing import Any

import mlx.core as mx
from mlx.optimizers import Optimizer  # <- same base as before


class KourkoutasSoftmaxFlex(Optimizer):
    """
    Adam variant with *layerâ€‘wise* dynamic Î²â‚‚ (â€œsunâ€‘spikeâ€) and optional
    softâ€‘max AMSGrad / trustâ€‘region features.

    --------------------------------------------------------------------------
    QUICK REFERENCE
    --------------------------------------------------------------------------
    decay        â€“ None: disable AMSGrad (unless max_ratio implies v_max);
                   1.0 : hard AMSGrad (nonâ€‘decreasing v_max);
                   (0,1): leakyâ€‘AMSGrad (softâ€‘max bound);
                   0.0 : degenerate (vÌ‚_t = v_t).
    max_ratio    â€“ Trustâ€‘region cap, applied as |Î”Î¸| â‰¤ lrÂ·max_ratio.
    adaptive_tinyâ€“ Adds an extra tiny term that scales with âŸ¨|Î¸âŸ©.
                   When False the classic Adam denominator âˆšv + eps is used.

    All three knobs default to â€œoffâ€, preserving vanilla Adam when
    decay=None, max_ratio=None, adaptive_tiny=False and Î²â‚‚ is fixed.
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ initialisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def __init__(
        self,
        learning_rate: float = 1e-3,
        beta1: float = 0.9,
        beta2_max: float = 0.999,
        beta2_min: float = 0.9,
        eps: float = 1e-8,
        alpha: float = 0.9,
        *,
        # --- diagnostics toggle
        diagnostics: bool = False,
        # --- tiny constants -------------------------------------------------
        tiny_spike: float = 1e-8,  # only inside sunâ€‘spike Î²â‚‚ logic
        tiny_denom: float = 1e-8,  # only in the Adam denominator
        # --- optional features ---------------------------------------------
        decay: float | None = None,  # softâ€‘max AMSGrad
        max_ratio: float | None = None,  # trustâ€‘region clip
        adaptive_tiny: bool = False,  # scale tiny_denom with |Î¸|
        # --- biasâ€‘correction & bookkeeping ---------------------------------
        bias_correction: str = "none",  # "none" | "beta2max" | "exact"
        warmup_steps: int = 0,
        layer_key_fn: Callable[[mx.array], Any] | None = None,
        schedulers=None,
    ):
        super().__init__(schedulers=schedulers)
        self._diag = diagnostics

        # ----- (possibly) scheduled scalars --------------------------------
        self._maybe_schedule("learning_rate", learning_rate)
        self._maybe_schedule("alpha", alpha)

        # ----- fixed hyperâ€‘parameters -------------------------------------
        self.beta1 = beta1
        self.beta2_max = beta2_max
        self.beta2_min = beta2_min
        self.eps = eps
        self.alpha = alpha

        self.tiny_spike = tiny_spike
        self.tiny_denom = tiny_denom

        # ----- feature toggles --------------------------------------------
        self.decay = decay
        self.max_ratio = max_ratio
        self.adaptive_tiny = adaptive_tiny

        # ----- biasâ€‘correction mode ---------------------------------------
        assert bias_correction in {"none", "beta2max", "exact"}
        self.bias_correction = bias_correction

        # ----- misc bookkeeping -------------------------------------------
        self.layer_key_fn = layer_key_fn
        self.warmup_steps = mx.array(int(warmup_steps), dtype=mx.int64)
        self.state["step"] = mx.array(0, dtype=mx.int64)
        self.state["_layer_stats"] = {}

        # lightâ€‘weight diagnostics (float32 scalars)
        if self._diag:  # allocate only if needed
            for name, init in [
                ("diag_max_ratio", 0.0),
                ("diag_denom_min", 1e9),
                ("diag_upd_norm_max", 0.0),
                ("diag_vhat_max", 0.0),
            ]:
                self.state[name] = mx.array(init, dtype=mx.float32)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ perâ€‘tensor slot initialiser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def init_single(self, p: mx.array, st: dict):
        st["m"] = mx.zeros_like(p)
        st["v"] = mx.zeros_like(p)
        st["beta2_cumprod"] = mx.ones([], dtype=p.dtype)

        # allocate `v_max` only if the user enabled *either* knob
        if (self.decay is not None) or (self.max_ratio is not None):
            st["v_max"] = mx.zeros_like(p)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main update â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def apply_gradients(self, grads: dict, params: dict):
        if not self._initialized:
            self.init(grads)

        # update scheduled scalars (lr, alpha, â€¦)
        for k, sched in self._schedulers.items():
            self.state[k] = sched(self.step)

        alpha = self.state["alpha"]
        self.state["step"] = self.step + 1

        # ---- gather perâ€‘group statistics ----------------------------------
        from mlx.utils import tree_map

        buckets: dict[Any, dict[str, Any]] = {}

        def collect(g, p, st):
            if g is None:
                return p
            gid = self._layer_id(p)
            bkt = buckets.setdefault(
                gid, {"sum_sq": mx.zeros([], dtype=g.dtype), "items": []}
            )
            bkt["sum_sq"] = mx.stop_gradient(bkt["sum_sq"] + mx.sum(mx.square(g)))
            bkt["items"].append((g, p, st))
            return p

        tree_map(collect, grads, params, self.state)

        # ---- perâ€‘bucket processing ----------------------------------------
        for gid, data in buckets.items():
            sum_sq, items = data["sum_sq"], data["items"]

            ls = self.state["_layer_stats"].setdefault(
                gid, {"grad_ema": mx.zeros([], dtype=sum_sq.dtype)}
            )

            g_norm = mx.sqrt(sum_sq)
            ls["grad_ema"] = alpha * ls["grad_ema"] + (1 - alpha) * g_norm
            g_ema = ls["grad_ema"]

            # dynamic Î²â‚‚ (â€œsunâ€‘spikeâ€)
            cond = (self.step <= self.warmup_steps).astype(g_norm.dtype)
            raw = g_norm / (g_ema + self.tiny_spike)
            sun = (1 - cond) * (raw / (1 + raw))

            beta2 = cond * 0.5 * (self.beta2_min + self.beta2_max) + (1 - cond) * (
                self.beta2_max - (self.beta2_max - self.beta2_min) * sun
            )

            lr = self.learning_rate.astype(sum_sq.dtype)
            b1 = self.beta1

            if self._diag:
                ls["last_spike"] = sun
                ls["last_beta2"] = beta2

            # ---- perâ€‘tensor inner loop ------------------------------------
            for g, p, st in items:
                m = st["m"] = b1 * st["m"] + (1 - b1) * g
                v = st["v"] = beta2 * st["v"] + (1 - beta2) * mx.square(g)

                # choose vÌ‚ (plain, AMSGrad, or softâ€‘max)
                v_hat = v
                if "v_max" in st:
                    if self.decay is not None:  # softâ€‘leak
                        st["v_max"] = mx.maximum(
                            mx.array(self.decay, v.dtype) * st["v_max"], v
                        )
                    else:  # hard AMSGrad
                        st["v_max"] = mx.maximum(st["v_max"], v)
                    v_hat = st["v_max"]

                # tiny term for the denominator
                if self.adaptive_tiny:
                    tiny_local = self.tiny_denom * mx.maximum(mx.mean(mx.abs(p)), 1.0)
                else:
                    tiny_local = mx.array(0.0, dtype=v.dtype)

                # bias correction flavour
                if self.bias_correction == "none":
                    denom = mx.sqrt(v_hat) + tiny_local + self.eps
                    upd = lr * m / denom
                else:
                    st["beta2_cumprod"] *= beta2
                    bc1 = 1.0 - b1**self.step
                    bc2 = (
                        1.0 - st["beta2_cumprod"]
                        if self.bias_correction == "exact"
                        else 1.0 - self.beta2_max**self.step
                    )
                    denom = mx.sqrt(v_hat / bc2) + tiny_local + self.eps
                    upd = (lr * (m / bc1)) / denom

                # optional trustâ€‘region clip
                if self.max_ratio is not None:
                    lim = lr * self.max_ratio
                    upd = mx.clip(upd, -lim, lim)

                # diagnostics (scalar fastâ€‘paths)
                if self._diag:
                    self.state["diag_max_ratio"] = mx.maximum(
                        self.state["diag_max_ratio"], mx.max(mx.abs(upd) / lr)
                    )
                    self.state["diag_denom_min"] = mx.minimum(
                        self.state["diag_denom_min"], mx.min(denom)
                    )
                    self.state["diag_upd_norm_max"] = mx.maximum(
                        self.state["diag_upd_norm_max"], mx.max(mx.abs(upd))
                    )
                    self.state["diag_vhat_max"] = mx.maximum(
                        self.state["diag_vhat_max"], mx.max(v_hat)
                    )

                # parameter update
                p -= upd

        return params

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers / utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _layer_id(self, p: mx.array):
        if self.layer_key_fn is not None:
            return self.layer_key_fn(p)
        if getattr(p, "name", None):
            return p.name.split(":")[0]
        return id(p)

    @property
    def step(self):
        return self.state["step"]

    # light snapshot for trainingâ€‘loop printouts
    def snapshot_diagnostics(self):
        keys = [
            "diag_denom_min",
            "diag_max_ratio",
            "diag_upd_norm_max",
            "diag_vhat_max",
        ]
        return {k: float(self.state[k].item()) for k in keys}

    def snapshot_sunspike_history(self):
        """
        Collect the mostâ€‘recent sunâ€‘spike and Î²â‚‚ value per layer *as Python floats*.

        Returns
        -------
        (spikes, betas) : tuple[list[float], list[float]]
            spikes[i] and betas[i] correspond to the same layer.
            Returns ([], []) if diagnostics are disabled.
        """
        if not self._diag:  # flag set in __init__()
            return [], []

        spikes, betas = [], []
        for st in self.state["_layer_stats"].values():
            if "last_spike" in st:  # present only when diagnostics on
                spikes.append(float(st["last_spike"].item()))
                betas.append(float(st["last_beta2"].item()))

        return spikes, betas
